"""Custom layers for Tensorflow:
- GlobalConv1D: 1D convolution that is invariant wrt permutations of the features
                Every convolutional kernel has two, one for the center (the value)
                the kernel is centered on, and one for the surround (all other values)
- TopKPool: selects the top-k values (equal to Maxpooling for k=1)

(c) matthias treder 2019
"""
from functools import partial

import tensorflow as tf
from tensorflow.python.keras.layers import Layer, Lambda


class GlobalConv1D(Layer):
    """
    GlobalConv1D
    """

    def __init__(self, filters, activation='linear', kernel_initializer='glorot_uniform',
                 bias_initializer='zeros', use_bias=True, **kwargs):
        self.bias = None
        self.kernel = None
        self.need_to_expand_dims = None
        self.filters = filters
        self.activation = activation
        self.kernel_initializer = kernel_initializer
        self.bias_initializer = bias_initializer
        self.use_bias = use_bias

        super(GlobalConv1D, self).__init__(**kwargs)

    def wrap_around_1d_kernel(self, width, n_input_chans):
        """Creates a convolutional kernel that wraps-around the features. It
        has a central weight. Surround weights are all the same. In order to have a
        wrap-around functionality, the true size of the filter will be 2*width + 1"""

        # c =  tf.get_variable("center", [1, n_input_chans, n_output_chans], initializer=self.initializer)
        # s =  tf.get_variable("surround", [1,n_input_chans, n_output_chans], initializer=self.initializer)

        # Create weight for center of convolutional kernel
        c = self.add_weight(name='kernel',
                            shape=(1, n_input_chans, self.filters),
                            # shape=(1, n_input_chans, tf.convert_to_tensor(self.filters)),
                            initializer=self.kernel_initializer,
                            trainable=True)

        # Create weight for surround of convolutional kernel
        s = self.add_weight(name='surround',
                            shape=(1, n_input_chans, self.filters),
                            initializer=self.kernel_initializer,
                            trainable=True)

        return tf.keras.backend.concatenate([s] * width + [c] + [s] * width, axis=0)

    def build(self, input_shape):
        """build"""
        self.need_to_expand_dims = len(input_shape) < 3
        n_input_chans = 1 if len(input_shape) < 3 else input_shape[2].value  # .value: casts shape to int

        # print('N_INPUT CHANS: ', n_input_chans)
        # Create wrap-around convolutional kernel
        self.kernel = self.wrap_around_1d_kernel(input_shape[1], n_input_chans)
        # print('expand dims: ', self.need_to_expand_dims)

        # Create bias weights (one bias term per output chan, since all the
        # input chans get combined during convolution)
        if self.use_bias:
            self.bias = self.add_weight(name='bias',
                                        shape=(1, 1, self.filters),
                                        initializer=self.bias_initializer,
                                        trainable=True)

        super(GlobalConv1D, self).build(input_shape)  # Be sure to call this at the end

    def call(self, x):
        """call"""
        # print('x: ', x)
        # print('kernel: ', self.kernel)
        # Create convolution operation
        if self.need_to_expand_dims:
            # If this layer comes directly after the input layer, and the input
            # shape was given as e.g. (nfeatures, ) it is just one-dimensional,
            # so we need to expand the dimensions
            op = tf.nn.conv1d(tf.expand_dims(x, -1), self.kernel, stride=1, padding='SAME')  # data_format='NCW')
        else:
            # If this layer comes not as first layer, or the input chans have been
            # explicitly defined as e.g. (nfeatures, 1) it is already two-dimensional,
            # so we do not need to expand the dimensions
            op = tf.nn.conv1d(x, self.kernel, stride=1, padding='SAME')  # data_format='NCW')

        # print('op: ', op)
        # print('bias: ', self.bias)
        if self.use_bias: op += self.bias

        # return tf.matmul(x,x)
        # out = K.dot(x,self.kernel)
        # out = K.dot(x, x)

        # return conv

        # Apply activation function and return
        return tf.keras.layers.Activation(self.activation)(op)
        # return out

    def compute_output_shape(self, input_shape):
        """
        compute_output_shape
        :param input_shape:
        :return:
        """
        return input_shape[0], self.output_dim


def convert(x, k):
    """convert"""
    # todo : tf.linalg.matrix_transpose OR tf.transpose
    return tf.convert_to_tensor(tf.linalg.transpose(tf.math.top_k(tf.linalg.transpose(x), k)[0]))


def TopKPool(k):
    """Pooling layer that retrieves the top K values from every input channel."""
    # x needs to be transposed since top k operates on the last dimension
    # which is channels in most cases. If the top K layer comes directly after
    # the input layer and it is one-dimensional e.g. shape = (n_features,) then
    # the input layer needs to be changed such that the second dimension (channels)
    # is specified as shape = (n_features,1)
    # After the operation, it is transposed back so that the rows correspond to
    # channels again
    fcn = partial(convert, k=k)
    return Lambda(fcn, trainable=False, name="top_{}_pool".format(k))
